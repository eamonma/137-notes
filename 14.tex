\input{~/preamble.tex}

\usepackage{blindtext}
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}


\usepackage{subfiles}

\sectionfont{\color{blue}\selectfont}
\subsectionfont{\color{green}\selectfont}

\newcommand{\eg}{\textbf{e.g.}~}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand\Ccancel[2][black]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}

\begin{document}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{definition}{Definition}[section]
{\LARGE \textbf{Unit 14: Series}}
\thispagestyle{empty}
\tableofcontents

\newpage
\clearpage
\setcounter{page}{1}
\section{Power series: an example}
\eg I want to define a function with this equation: \[g(x) = \sum_{n = 1}^{\infty} \frac{x^n}{n3^n}\]
For which \(x \in \R\) is \(g(x)\) convergent? We can use the Ratio Test.
\begin{align*}
    \text{Call } a_n                                       & = \frac{x^n}{n3^n}                                                      \\
    L = \lim_{n\to\infty} \cfrac{\abs{a_{n+1}}}{\abs{a_n}} & = \cfrac{\abs{\cfrac{x^{n+1}}{(n+1) 3^{n+1}}}}{\abs{\cfrac{x^n}{n3^n}}} \\
                                                           & = \frac{\abs{x}}{3}
\end{align*}
\begin{itemize}
    \item If \(\abs{x} < 3\), then \(L = \frac{\abs{x}}{3} < 1\). By Ratio Test, \(g(x)\) is absolutely convergent.
    \item If \(\abs{x} > 3\), then \(L = \frac{\abs{x}}{3} > 1\). By Ratio test, \(g(x)\) is divergent.
\end{itemize}
We don't know what happens at \(x = -3\) or \(x = 3\) yet. \\
\begin{align*}
    g(3)  & = \sum_{n = 1}^{\infty} \frac{3^n}{n3^n} = \frac{1}{n}  = \infty \text{ (p-series with \DS{p = 1})} \\
    g(-3) & = \sum_{n = 1}^{\infty} \frac{(-3)^n}{n3^n} = \frac{(-1)^n}{n} \text{ convergent (by \textsc{ast})} \\
\end{align*}
Then, at \(x = -3\), \(g(x)\) is conditionally convergent, and \(3\), it is divergent. \\
To answer the original question, the domain of \(g = [-3, 3)\) = the \textsc{interval of convergence}. \(3\) = the \textsc{radius of convergence}.
\newpage



\section{Power series: the main theorem}
\subsection*{Motivation}
\begin{itemize}
    \item Polynomials are nice
    \item What about ``infinite polynomials''? \\
          \[f(x) = c_0 + c_1x + c_2x^2 + c_3x^3 + \dots\]
          \[\text{or} \quad f(x) = c_0 + c_1 (x - a) + c_2 (x - a)^2 + c_3 (x - a)^3 + \dots\]
    \item \eg: \begin{itemize}
              \item \(g(x) = \sum_{n = 1}^{\infty} \frac{x^n}{n3^n}\) has domain \([-3, 3)\)
              \item \(h(x) = \sum_{n = 0}^{\infty} x^n\) has domain \((-1, 1)\)
          \end{itemize}
\end{itemize}
\df{
    Let \(a \in \R\). \\
    A \underline{power series centered at \(a\)} is a function \(f\) defined by an equation like \begin{align*}
        f(x) = \sum_{n = 0}^{\infty} c_n(x-a)^n = c_0 + c_1(x - a) + c_2(x - a)^2 + \dots
    \end{align*}
    where \(c_0, c_1, c_2, \dots \in \R\).
}

\begin{center}
    Domain \(f = \{x \in \R ~ : ~ \text{teh series \(f(x)\) is convergent}\}\)
\end{center}
Note: \(a \in\) Domain \(f\) \\
Ultimate goal: write common functions as power series.
\thm{
    Let \(f(x) = \sum_{n = 0}^{\infty} c_n (x - a)^n\) be a power series centered at \(a \in \R\). \begin{enumerate}
        \item The domain of \(f\) is an interval centered at a: \begin{align*}
                   & (a - R, a + R) \quad (a - R, a + R] \quad \R    \\
                   & [a - R, a + R] \quad [a - R, a + R) \quad \{a\}
              \end{align*} \begin{itemize}
                  \item We call this domain the \underline{interval of convergence} (\textsc{ic}) of \(f\).
                  \item We call its radius the \underline{radius of convergence}. \(\quad 0 \leq R \leq \infty\)
              \end{itemize}
        \item \begin{itemize}
                  \item In the \textbf{interior} of the \textsc{ic}, the series is absolutely convergent.
                  \item In the \textbf{exterior} of the \textsc{ic}, the series is divergent.
                  \item At the endpoints (if any), anything may happen.
              \end{itemize}
        \item In the interior of the \textsc{ic}, power series can be ``treated like polynomials''. They can be added, multiplied, composed\dots \\
              In particular, they can be differentiated or integrated ``term by term'', and this does not change the radius of convergence.
    \end{enumerate}
}

\begin{alignat*}{2}
    f(x)                   & = \sum_{n = 0}^{\infty} c_n x^n             &  & = c_0 + c_1x + c_2x^2 + c_3x^3 + \dots                 \\
    f'(x)                  & = \sum_{n = 0}^{\infty} c_n n x^{n-1}       &  & = c_1 + 2c_2x + 3c_3x^2 + \dots                        \\
    \int_{0}^{x} {f(t)} dt & = \sum_{n = 0}^{\infty} \frac{x^{n+1}}{n+1} &  & = c_0x + c_1 \frac{x^2}{2} + c_2 \frac{x^3}{3} + \dots
\end{alignat*}

\textbf{Goals} \begin{enumerate}
    \item Write as many functions as possible as power series \\
          \(\longrightarrow\) Taylor series
    \item Use that to make limits, integrals, estimations, differential equations, physics,\dots easier.
\end{enumerate}

\newpage
\section{Taylor polynomials---the definition with the limit}
Goal: approximate functions with polynomials. \\
\(f\): function \\
\(a \in\) Domain \(f\) \\
\(P\): polynomial \\

I want \(P(x) \approx f(x)\) when \(x\) is close to \(a\). Example: the tangent line. But what is a ``good approximation near a''? \\
\(R\): "remainder" or "error" \(\quad R(x) = f(x) - P(x)\). I want \(R\) to be small. This means we need \(\lim_{x \to a} R(x) = 0 \) ``fast''. For instance, there are many lines with remainder \(0\), but the tangent line's remainder approaches \(0\) the ``fastest''. But how do we measure how fast the limit is? \\
We notice that the larger exponent polynomials approach \(0\) faster. We compare the remainder with powers of \(x\).
\df{
    Let \(f\) and \(g\) be continuous functions at \(0\). \\
    Let \(n \in \N\). \\
    We say that \underline{\(g\) is an approximation for \(f\) near \(0\) of order \(n\)} when
    \begin{align*}
        \lim_{x\to0} \frac{f(x) - g(x)}{x^n} = 0
    \end{align*}
}
This means that \(f(x) = g(x) + R(x)\) and as \(x \to 0\), \begin{align*}
    R(x) \to 0 \quad \text{faster than} \quad x^n \to 0
\end{align*}
\df{
    Let \(a \in \R\). Let \(f\) and \(g\) be continuous functions at \(a\). \\
    Let \(n \in \N\). \\
    We say that \underline{\(g\) is an approximation for \(f\) near \(a\) of order \(n\)} when
    \begin{align*}
        \lim_{x\to a} \frac{f(x) - g(x)}{(x-a)^n} = 0
    \end{align*}
}

\subsection*{First definition of Taylor polynomial}
\df{
    Let \(a \in \R\). \\
    Let \(f\) be a continuous function defined at and near \(a\). \\
    Let \(n \in \N\). \\
    The \underline{\(n\)-th Taylor polynoial for \(f\) at \(a\)} is a polynomial \(P_n\) \begin{itemize}
        \item \dots that is an approximation for \(f\) near \(a\) of order \(n\): \begin{align*}
                  \lim_{x\to a} \frac{f(x) - P_n(x)}{(x-a)^n} = 0
              \end{align*}
        \item with degree at most n
    \end{itemize}
}



\newpage
\section{Taylor polynomials---the definition with the derivatives}
\df{
    A function \(f\) is called \dots \begin{itemize}
        \item \(C^0\) when \(f\) is continuous
        \item \(C^1\) when \(f'\) exists and is continuous
        \item \(C^2\) when \(f'\) and \(f''\) exist and are continuous
        \item \dots
        \item \(C^n\) when \(f', f'', \dots, f^{(n)}\) exist and are continuous
        \item \(C^\infty\) when all derivatives exist (and are continuous)
    \end{itemize}
}
For now, assume \(f\) and \(g\) are \(C^\infty\). Can I transform the condition \begin{align*}
    \lim_{x\to a} \frac{f(x) - g(x)}{(x-a)^n} = 0
\end{align*}
into a condition about their derivatives?

\begin{itemize}
    \item Call \DS{L = \lim_{x\to a} \frac{f(x) - g(x)}{(x-a)^n}} \begin{itemize}
              \item If \(f(a) - g(a) \neq 0\), then ``\(L = \frac{\text{not } 0}{0} = \pm \infty\)''.
              \item So, assume \(f(a) = g(a)\). We get \(0/0\). Use L'hôpital's.
          \end{itemize}
    \item \DS{L \stackrel{*}{=} \lim_{x\to a} \frac{f'(x) - g'(x)}{n(x-a)^{n-1}}} \begin{itemize}
              \item If \(f'(a) - g'(a) \neq 0\), then ``\(L = \frac{\text{not } 0}{0} = \pm \infty\)''
              \item So, assume \(f'(a) = g'(a)\). We get \(0/0\). Use L'hôpital's.
          \end{itemize}
    \item \DS{L \stackrel{*}{=} \lim_{x\to a} \frac{f''(x) - g''(x)}{n(n-1)(x-a)^{n-2}} \dots}
    \item After using L'hôpital's rule \(n\) times, we get \begin{align*}
              L \stackrel{*}{=} \lim_{x\to a} \frac{f^{(n)}(x) - g^{(n)}(x)}{n!} = \frac{f^{(n)}(a) - g^{(n)}(a)}{n!} \qquad \text{since the derivatives are continuous}
          \end{align*}
\end{itemize}
\begin{align*}
    L = 0 \Longleftrightarrow \begin{cases}
        f(a)         & = g(a)         \\
        f'(a)        & = g'(a)        \\
        \dots                         \\
        f^{(n-1)}(a) & = g^{(n-1)}(a) \\
        f^{(n)}(a)   & = g^{(n)}(a)   \\
    \end{cases}
\end{align*}
I have used that \(f\) and \(g\) were \(C^n\).

\thm{
    Let \(a \in \R\). Let \(n \in \N\). \\
    Let \(f\) and \(g\) be \(C^n\) functions at \(a\). \\
    The following are equivalent: \begin{enumerate}
        \item \DS{\lim_{x\to a} \frac{f(x) - g(x)}{(x-a)^n} = 0} \\ (``\(g\) is a good approximation for \(f\) near \(a\)'')
        \item \DS{f(a) = g(a), f'(a) = g'(a), \dots, f^{n}(a) = g^{n}(a)} \\ (``\(g\) and \(f\) have the same first few derivatives at \(a\)'')
    \end{enumerate}
}
This proof could be made more formal by using induction.

\df{
    Let \(a \in \R\). \\
    Let \(n \in \N\). \\
    Let \(f\) be a \(C^n\) function at \(a\). \\
    The \underline{\(n\)-th Taylor polynomial for \(f\) at \(a\)} is \begin{itemize}
        \item a polynomial \(P_n\) such that
        \item \(P_n(a) = f(a), P_n'(a) = f'(a), \dots P_n^{(n)}(a) = f^{(n)}(a)\)
        \item with degree at most \(n\).
    \end{itemize}
}

This definition is more useful that the original, limit definition for constructing these polynomials. However, we should keep the original definition in mind---it tells us why Taylor polynomials make good approximations. \\
Notice that this is not a completely new idea: the first Taylor polynomial according to this definition is \(y = P_1(x)\), or the tangent line.



\newpage
\section{Taylor polynomials---the formula}
\bo{
    \textbf{Recall:} \\
    The \(n\)-th \underline{Taylor polynomial \(P_n\)} for the function \(f\) at \(a \in \R\)\dots \begin{itemize}
        \item is an approximation for \(f\) near \(a\) of order \(n\): \begin{align*}
                  \lim_{x\to a} \frac{f(x) - P_n(x)}{(x-a)^n} = 0
              \end{align*}
        \item equivalently, has same value and first \(n\) derivatives as \(f\) at \(a\): \begin{align*}
                  P_n(a) = f(a), ~ P_n'(a) = f'(a), ~ \dots, ~ P_n^{(n)}(a) = f^{(n)}(a)
              \end{align*}
    \end{itemize}
}
For simplicity, begin with: \\
\textbf{Case \(a = 0\)} \\
\begin{align*}
    P_n(x) = c_0 + c_1x + c_2x^2 + c_3x^3 + \dots + c_n x^n
\end{align*} \begin{align*}
    P_n(0)       & = f(0)       \\
    P_n'(0)      & = f'(0)      \\
    P_n''(0)     & = f''(0)     \\
    \dots                       \\
    P_n^{(n)}(0) & = f^{(n)}(0)
\end{align*}
So, \begin{align*}
    P_n^{(k)}(0) & = k! \cdot c_k = f^{(k)} \\
    c_k = \frac{f^{(k)}(0)}{k!}             \\
    \boxed{P_n(x) = \sum_{k = 0}^{n} \frac{f^{(k)}(0)}{k!} x^k}
\end{align*}
Now, how do we \\
\textbf{Generalize \(a\)?}
Instead of \begin{align*}
    P_n(x) = c_0 + c_1x + c_2x^2 + c_3x^3 + \dots + c_n x^n
\end{align*}
write \begin{align*}
    P_n(x) = b_0 + b_1(x - a) + b_2(x - a)^2 + b_3(x - a)^3 + \dots + b_n (x - a)^n
\end{align*}
We obtain \begin{align*}
    \boxed{P_n(x) = \sum_{k = 0}^{n} \frac{f^{(k)}(a)}{k!} (x-a)^k}
\end{align*}
\df{
    \textbf{Third definition of Taylor polynomial}
    \begin{itemize}
        \item Let \(a \in \R\)
        \item Let \(n \in \N\)
        \item Let \(f\) be a \(C^n\) function at \(a\).
    \end{itemize}
    The \underline{\(n\)-th Taylor polynomial} for \(f\) at \(a\) is \begin{align*}
        P_n(x) = \sum_{k = 0}^{n} \frac{f^{(k)}(a)}{k!} (x-a)^k
    \end{align*}
}
Notes: \begin{itemize}
    \item degree \(P_n \leq n\)
    \item The Taylor polynomials of a function are unique
\end{itemize}
We can see that the higher degree the Taylor polynomial is, the more accurate the approximation.
\df{
    \textbf{Taylor series}
    \begin{itemize}
        \item Let \(a \in \R\)
        \item Let \(f\) be a \(C^\infty\) function at \(a\)
    \end{itemize}
    The \underline{Taylor series} for \(f\) at \(a\) is the power series \begin{align*}
        S(x) = \sum_{k = 0}^{\infty} \frac{f^{(k)}(a)}{k!} (x - a)^k
    \end{align*}
}
Equivalently, \(\forall k \in \N,~ S^{(k)}(a) = f^{(k)}(a)\) \\
Ideal case: \(f(x) = S(x)\); we call such functions \emph{analytic}. \\
Note: ``Maclaurin series'' means ``Taylor series at \(0\)''.



\newpage
\section{The four main Maclaurin series}
\begin{itemize}
    \item The \underline{\(n\)-th Taylor polynomial} for \(f\) at \(a\): \begin{align*}
              P_n(x) = \sum_{k = 0}^{n} \frac{f^{(k)}(a)}{k!} (x - a)^k
          \end{align*}
    \item The \underline{Taylor series} for \(f\) at \(a\): \begin{align*}
              S(x) = \sum_{k = 0}^{\infty} \frac{f^{(k)}(a)}{k!} (x - a)^k
          \end{align*}
    \item \underline{Maclaurin series} is a Taylor series at \(0\)
    \item \(f\) is \underline{analytic} when \(f(x) = S(x)\)
\end{itemize}
\eg Maclaurin series for \(f(x) = e^x\) \begin{itemize}
    \item For all \(k \in \N, ~ f^{(k)}(x) = e^x, ~ f^{(k)}(0) = 1\). \begin{align*}
              S(x) & = \sum_{k = 0}^{\infty} \frac{f^{(k)}(a)}{k!} (x - a)^k = \sum_{k = 0}^{\infty} \frac{x^k}{k!} \\
                   & = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \dots
          \end{align*}
\end{itemize}


\eg Taylor series for \(f(x) = e^x\) at \(c\) \\
To write \(f(x) = e^x\) in terms of powers of \((x-c)\) \dots, we can use the substitution \begin{center}
    \fbox{\(u=x-c\)}
\end{center}
\dots write \(f\) in terms of powers of \(u\) instead \begin{align*}
    e^x = e^{c+u} = e^c \cdot e^u = e^c \sum_{n = 0}^{\infty} \frac{u^n}{n!} \\
    \boxed{e^x = \sum_{n = 0}^{\infty} \frac{e^c}{n!}(x-c)^n}
\end{align*}


\eg Maclaurin series for \(g(x) = \sin x\)
\begin{multicols}{2}
    \begin{itemize}
        \item \(g(x) = \sin x\)
        \item \(g'(x) = \cos x\)
        \item \(g''(x) = -\sin x\)
        \item \(g'''(x) = -\cos x\)
        \item \(g^4(x) = \sin x\)
    \end{itemize}
    \begin{itemize}
        \item \(g(0) = 0\)
        \item \(g'(0) = 1\)
        \item \(g''(0) = 0\)
        \item \(g'''(0) = -1\)
        \item \(\dots\)
    \end{itemize}
\end{multicols}
\begin{align*}
    S(x) & = \sum_{n = 0}^{\infty} g^{(0)}(0)\frac{x^n}{n!}                          \\
         & = \frac{x}{1!} - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \dots \\
         & = \sum_{m = 0}^{\infty} (-1)^m \frac{x^{2m+1}}{(2m+1)!}
\end{align*}
\(\sin\) is analytic, so \(\sin x = S(x)\)


\eg Maclaurin series for \(h(x) = \cos x\)
Left as exercise

\subsection*{The four main Maclaurin series}
\bo{
    \begin{alignat*}{2}
        e^x           & = \sum_{n = 0}^{\infty} \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \dots \quad          &  & \text{for all } x       \\
        \sin x        & = \sum_{n = 0}^{\infty} (-1)^n \frac{x^{2n+1}}{(2n+1)!} = x - \frac{x^3}{3!} + \frac{x^5}{5!} + \dots \quad              &  & \text{for all } x       \\
        \cos x        & = \sum_{n = 0}^{\infty} (-1)^n \frac{x^{2n}}{(2n)!} = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \dots \quad &  & \text{for all } x       \\
        \frac{1}{1-x} & = \sum_{n = 0}^{\infty} x^n = 1 + x + x^2 + x^3 + x^4 + \dots \quad                                                      &  & \text{for } \abs{x} < 1
    \end{alignat*}
}


\newpage
\section{Analytic functions and the remainder theorems}
An analytic function is equal to its Taylor series\dots \begin{itemize}
    \item \dots centered where?
    \item \dots on which domain?
\end{itemize}

\df{
    Let \(f\) be a \(C^\infty\) function defined on an open interval \(I\). \begin{itemize}
        \item Let \(a \in I\). Let \(S_a(x)\) be the Taylor series of \(f\) at \(a\). \\
              We say that \fbox{\(f\) is analytic at \(a\)} when \begin{align*}
                  \text{\(\exists \) an open interval \(J_a\) centered at \(a\) such that} \\
                  \forall x \in J_a, \quad f(x) = S_a(x)
              \end{align*}
        \item We say that \fbox{\(f\) is analytic} when \begin{align*}
                  \forall a \in I, \quad \text{\(f\) is analytic at \(a\)}
              \end{align*}
    \end{itemize}
}
We often carelessly summarize this as ``the function equals its Taylor series''. \\
\subsection*{Some results about analytic functions (that I won't prove)} \begin{enumerate}
    \item Polynomials are analytic
    \item Sums, products, quotients* and composition of analytic functions are analytic
    \item Derivatives and antiderivatives of analytic functions are analytic
    \item In the interior of the interval of convergence, a power series ``can be manipulated like a polynomial''
    \item The Taylor series of a function at a point is unique
\end{enumerate}
\bo{
    \textbf{Goal}: prove that \begin{align*}
        f(x) = e^x \quad \text{and} \quad g(x) = \sin x
    \end{align*}
    are analytic.
}
Why only these two functions? Other elementary functions can be written in terms of these two functions, polynomials, and the operations given.
\subsection*{How will we prove a function is analytic?}
Let \(f\) be a function defined on an open interval \(I\). Let \(a \in I\). \begin{itemize}
    \item If \(f\) is \(C^n\), we can write the \(n\)-th Taylor polynomial \(P_n\) at \(a\): \begin{align*}
              P_n(x) = \sum_{k = 0}^{n} \frac{f^{(k)}(a)}{k!} (x-a)^k, \quad \boxed{f(x) = P_n(x) + R_n(x)}
          \end{align*}
    \item If \(f\) is \(C^\infty\), we can write the Taylor series \(S\) at \(a\): \begin{align*}
              S(x) = \sum_{k = 0}^{\infty} \frac{f^{(k)}(a)}{k!} (x - a)^k, \quad \boxed{\lim_{n\to \infty} P_n(x) = S(x)}
          \end{align*}
    \item \(f\) is analytic when, in addition, \begin{align*}
              f(x) = S(x) \quad \boxed{\text{This is equivalent to } \lim_{n\to \infty} R_n(x) = 0}
          \end{align*}
          Didn't we know this already? No.
          \begin{enumerate}
              \item We know, from the definition of Taylor polynomial: \begin{align*}
                        \text{for fixed } n, \quad \lim_{x\to a} \frac{R_n(x)}{(x-a)^n} = 0
                    \end{align*}
              \item We want, for the function to be analytic: \begin{align*}
                        \text{for fixed } x, \quad \lim_{n\to\infty} R_n(x) = 0
                    \end{align*}
          \end{enumerate}
\end{itemize}
How to estimate the remainder of a Taylor polynomial?
\thm{
    \textbf{Typical remainder theorem} \begin{itemize}
        \item \textsc{if} ( hypotheses )
        \item \textsc{them} ( some formula for \(R_n(x)\) )
    \end{itemize}
}
There are at least three versions: \begin{enumerate}
    \item Lagrange's form
    \item Cauchy's form
    \item Integral form
\end{enumerate}
They are all proven with the \textsc{mean value theorem} or \textsc{rolle's theorem}.
\thm{
    \textbf{Lagrange's remainder theorem} \begin{itemize}
        \item Let \(I\) be an open interval. Let \(a \in I\).
        \item Let \(n \in \N\).
        \item Let \(f\) be a \(C^{n+1}\) function on \(I\).
        \item Let \(P_n(x) = \sum_{k = 0}^{n}  \frac{f^{(k)}(a)}{k!} (x-a)^k \) be its \(n\)-th Taylor polynomial.
        \item Let \(R_n(x) = f(x) - P_n(x)\) be the remainder.
    \end{itemize}
    \textsc{then} \begin{align*}
        \exists \xi \text{ between \(a\) and \(x\) such that } R_n(x) = \frac{f^{(n+1)} (\xi)}{(n+1)!}(x-a)^{n+1}
    \end{align*}
}

\subsection*{How is a remainder theorem useful?}
\begin{enumerate}
    \item To prove a function is analytic: \begin{align*}
              \lim_{n\to\infty} R_n(x) = 0
          \end{align*}
    \item To estimate: \\
          Approximate \(f(x)\) by \(P_n(x)\) and bound the error.
\end{enumerate}



\newpage
\section{A proof that the exponential function is analytic}
\textbf{Goal: prove that \(f(x) = e^x\) is analytic at \(0\)}
\begin{itemize}
    \item \(f(x) = e^x,~ a = 0\).
          Since this function is \(C^\infty\)
          \begin{align*}
              e^x = P_n(x) + R_n(x)
          \end{align*}
    \item \(P_n(x) = \sum_{k = 0}^{n} \frac{x^k}{k!}\) is the \(n\)-th Taylor polynomial.
    \item \(R_n(x)\) is the \(n\)-th remainder. \begin{center}
              We need to show \fbox{\(\forall x \in \R,~ \lim_{n\to\infty} R_n(x) = 0\)}
          \end{center}
    \item Then, as a consequence, \begin{align*}
              e^x = \sum_{k = 0}^{\infty} \frac{x^k}{k!}
          \end{align*}
\end{itemize}
To prove that the exponential equals its Maclaurin series, all we need to show is that the remainders approach \(0\) as \(n \to \infty\).
\begin{proof}~
    \begin{itemize}
        \item Fix \(x \in \R\).
        \item Use Lagrange's remainder theorem for \(f(x) = e^x\) and \(a = 0\). \\
              Then, \((\forall n \in \N) \quad \exists \xi \) between \(0\) and \(x\) such that \begin{align*}
                  R_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}x^{n+1} = \frac{e^\xi}{(n+1)!}x^{n+1}
              \end{align*}
        \item Then \dots?\begin{align*}
                  \lim_{n\to\infty} R_n(x) = \lim_{n\to\infty} \sbr{e^\xi\frac{x^{n+1}}{(n+1)!}} = \Ccancel[red]{
                      e^\xi \sbr{\lim_{n\to\infty} \frac{x^{n+1}}{(n+1)!}}
                  }
              \end{align*}
        \item No. {\color{red} \(\xi\) is not a constant.} I depends on \(n\) and \(x\). \\
              All we know about \(\xi\) is that it is between \(0\) and \(x\).
        \item \underline{Case 1: \(x > 0\).} Then \(0 < \xi < x\), so \begin{align*}
                  0 \leq R_n(x) = e^\xi \frac{x^{n+1}}{(n+1)!} \leq e^x \frac{x^{n+1}}{(n+1)!}
              \end{align*} \begin{itemize}
                  \item From \textsc{big theorem} \(\lim_{n\to\infty} \frac{x^{n+1}}{(n+1)!} = 0\)
                  \item From \textsc{squeeze theorem} \(\lim_{n\to\infty} R_n(x) = 0\)
              \end{itemize}
        \item \underline{Case 2: \(x < 0\).} (Left as exercise)
    \end{itemize}
\end{proof}


\newpage
\section{How to write functions as power series quickly}
\subsection*{The slow method}
\begin{enumerate}
    \item Start with a \(C^\infty\) function \(f\)
    \item Obtain the Taylor series at \(a\): \begin{align*}
              S(x) = \sum_{k = 0}^{\infty} \frac{f^{(k)}(a)}{k!} (x-a)^k
          \end{align*}
    \item Use the remainder theorems to prove \begin{align*}
              \lim_{n\to\infty} R_n(x) = 0
          \end{align*}
    \item Then \(f(x) = S(x)\)
\end{enumerate}
\subsection*{The better method}
\bo{
    \textbf{The tool} \\
    In the interior of the interval of convergence, a power series can be manupulated like a polynomial.
}
\eg Write these functions as power series: \begin{enumerate}
    \item \(f(x) = e^{-x} \quad \text{at } a = 0\)
    \item \(f(x) = x^3 \sin x^2 \quad \text{at } a = 0\)
    \item \(f(x) = \frac{1}{x} \quad \text{at } a = 0\)
    \item \(f(x) = \ln (1+x) \quad \text{at } a = 0\)
\end{enumerate}
\textbf{Solutions}
\begin{enumerate}
    \item Write \(f(x) = e^{-x} \text{ as a power series at } a = 0\) \\
          This is close to \(e^x\), which we already know how to write as a power series. \begin{align*}
              e^x = \sum_{n = 0}^{\infty} \frac{x^n}{n!} \quad \text{ for all } x
          \end{align*}
          We want \(e^{-x}\), so we just replace \(x\) with \(-x\) everywhere. \begin{align*}
              e^{-x} = \sum_{n = 0}^{\infty} \frac{(-x)^n}{n!} = \sum_{n = 0}^{\infty} \frac{(-1)^n}{n!} x^n \quad \text{for all } x
          \end{align*}
    \item Write \(f(x) = x^3 \sin x^2 \text{ as a power series at } a = 0\) \\
          This is close to \(\sin x\). \begin{align*}
              \sin x   & = \sum_{n = 0}^{\infty} \frac{(-1)^n}{(2n+1)!} x^{2n+1} \quad \text{ for all } x                                       \\
              \sin x^2 & = \sum_{n = 0}^{\infty} \frac{(-1)^n}{(2n^2 + 1)!} x^{2(2n+1)} = \sum_{n = 0}^{\infty} \frac{(-1)^n}{(2n+1)!} x^{4n+2} \\
              f(x)     & = x^3 \sin x^2 = \sum_{n = 0}^{\infty} \frac{(-1)^n}{(2n+1)!} x^{4n+5} \quad \text{ for all    } x
          \end{align*}
    \item Write \(f(x) = \frac{1}{x}\) as a power series at \(3\) \\
          Change of variable \fbox{\(u = x - 3\)} \begin{align*}
              \frac{1}{x} & = \frac{1}{u+3} \qquad \br{\frac{1}{1-x} = \sum_{n = 0}^{\infty} x^n \quad \text{ for } \abs{x} < 1}                    \\
                          & = \frac{1}{3} \frac{1}{1 + (u/3)} = \frac{1}{3} \frac{1}{1-(-u/3)} = \frac{1}{3} \sum_{n = 0}^{\infty} (-\frac{u}{3})^n \\
                          & = \sum_{n = 0}^{\infty} \frac{(-1)^n}{3^{n+1}} u^n = \sum_{n = 0}^{\infty} \frac{(-1)^n}{3^{n+1}} (x-3)^n
          \end{align*}
          Valid when \(\abs{-\frac{u}{3}} < 1\). This is the same thing as saying that \(\abs{u} < 3\). \(\abs{x - 3} < 3\).
\end{enumerate}


\newpage
\section{Lograithm as a power series}
How to write \(\log\) as power series?
Let \(g(x) = \ln x\). Domain \(g=(0, \infty)\)
\begin{itemize}
\item Option 1: Taylor series centered at \(a > 0\) of \(g(x) = \ln x\).
  \item Write \(f(x) = \ln (1+x)\) as power series at \(0\) .
    \begin{align*}
      f'(x)&= \frac{1}{1+x} \quad \br{\frac{1}{1-x} = \sum_{n = 0}^{\infty} x^{n} \quad \text{for } \abs{x} < 1 }\\
       &= \frac{1}{1-(-x)} = \sum_{n = 0 }^{\infty} (-x)^{n}  = \sum_{n = 0}^{\infty} (-1)^{n} x^{n} \quad \br{\text{for } \abs{x} < 1  }\\
      f(x) &= \sum_{n = 0}^{\infty} (-1)^{n} \frac{x^{n+1}}{n+1} + C \quad \br{\text{for} \abs{x} < 1}\\
      \intertext{Evaluate at \(x = 0\):}
      f(0) = 0 + C \quad C = 0 \\
      \ln (1+x) & = \sum_{n = 0}^{\infty} (-1)^{n} \frac{x^{n+1}}{n+1}\\
           & = \sum_{m = 1}^{\infty} (-1)^{m - 1} \frac{x^{m}}{m}
    \end{align*}
\end{itemize}
Remember, true only for \(\abs{x} < 1\).




\newpage
\section*{Taylor applications: estimations}
\eg Estimate \(\sqrt{e}\) with error  \(< 0.001\).
\begin{itemize}
    \item Let \(f(x) = e^{x}\). Want \(f(\frac{1}{2})\).
    \item \(\frac{1}{2} = \sum_{k = 0}^{\infty} \frac{1}{k!}\frac{1}{2^{k}} = P_{n} (\frac{1}{2}) + R_{n}(\frac{1}{2})\)
    \item \(P_{n}(\frac{1}{2}) = \sum_{k = 0}^n \frac{1}{k!} \frac{1}{2^{k}}\)
        \item Want \(\abs{R_{n}(\frac{1}{2})}< 0.001\)

\end{itemize}
    \subsection*{Use Lagrange's remainder theorem!}
    \begin{align*}
\exists \xi \in (0, \frac{1}{2}) \quad \text{such that} \quad R_{n} \br{\frac{1}{2}} = \frac{f^{(n+1)} (\xi)}{(n+1)!} \sbr{\frac{1}{2} - 0}^{n+1} \\
0 < R_n(\frac{1}{2}) < \frac{  e ^{\frac{1}{2}}  }{(n+1)!} \frac{1}{2 ^{n+1} } < \frac{2}{2 ^{n+1} (n+1) !} < {\color{red} 0.001}
    \end{align*}
Need \(2 ^{n} (n + 1) ! > 1000\); \(n = 4\) works: \(2^5 \cdot 5! = 16 \cdot 120 > 1000\).
Estimation for \(\sqrt{e}\): \begin{align*}
P_4 (\frac{1}{2}) = \sum_{k = 0}^{4} \frac{1}{k! 2^k} = \dots = \frac{211}{128} \approx 1.65843\dots
\end{align*}



\newpage
\section{Taylor applications: integrals}
\begin{itemize}
    \item Compute \(I = \int_{0}^{3} {e^{-x^2}} dx \).
    \item \(e^x = \sum_{n = 0}^{\infty} \frac{x^n}{n!}\) for all \(x\).
    \item Since there is no domain restriction, we replace \(x\) with \(-x^2\). \begin{align*}
        e^{-x^2} = \sum_{n = 0}^{\infty} \frac{(-x^2)}{n!} \qquad \text{for all } x
    \end{align*}
\end{itemize}
\newpage
\section{Taylor applications: limits}
\begin{align*}
 & \lim_{x\to 0} \dfrac{\sin x - x}{x^3}\\
=& \lim_{x\to 0} \dfrac{\sbr{x - \dfrac{1}{3!}x^3 + \dfrac{1}{5!} x^5 - \dfrac{1}{7!}x^7}+ \dots - x}{x^3} \\
=& \lim_{x\to 0} \dfrac{\sbr{-\dfrac{1}{3!}x^3 + \dfrac{1}{5!}x^5 + \dots}}{x^3} = \lim_{x\to 0} \sbr{-\dfrac{1}{6}+ \dfrac{1}{120}x^2+ \dots} \\
=& - \dfrac{1}{6}
\end{align*}
We can notice here that the only term that really mattered was the first.

\eg 2: \begin{align*}
& \lim_{x \to 0} \frac{3x^2 - e^{x^2} + \cos (2x)}{x\sin x - \ln (1+x^2)}\\
\end{align*}
Numerator
\begin{itemize}
    \item (+) \({\color{blue} 3x^2}\)
    \item (-) \(e^{x^2} = {\color{red} 1 } + {\color{blue} x^2 } + {\color{green} \frac{1}{2!} (x^2)^2 } + \frac{1}{3!} (x^2)^3 + \dots\)
    \item (+) \(\cos (2x) = {\color{red} 1 } - {\color{blue} \frac{1}{2!}(2x)^2 } + {\color{green} \frac{1}{4!} (2x)^4 }   - \frac{1}{6!} (2x)^6 + \dots\)
\begin{align*}
3x^2 - e^{x^2} + \cos(2x) = {\color{red} \sbr{-1 + 1} } + {\color{blue} \sbr{3 - 1 - \frac{4}{2}}x^2 } + {\color{green} \sbr{-\frac{1}{2} + \frac{2^4}{4!}}x^4 } + \dots = \frac{1}{6} x^4 + \text{higher order terms}
\end{align*}
\end{itemize}
Denominator
\begin{itemize}
    \item (+) \(x \sin x = x \sbr{{\color{red} x } - {\color{blue} \frac{1}{3!} x^3 } + \frac{1}{5!} x^5} - \dots\)
    \item (-) \(\ln (1+ x^2) = {\color{red} x^2 } - {\color{blue} \frac{1}{2}(x^2)^2 } + \frac{1}{3}(x^2)^3 - \dots\)
    \begin{align*}
    x\sin x - \ln (1+x^2) = {\color{red} \sbr{1 - 1}x^2 } + {\color{blue} \sbr{-1\frac{1}{6} + \frac{1}{2}} x^4 } + \dots = \frac{1}{3} x^4 + \text{hiogher order terms}
    \end{align*}
\end{itemize}
\begin{align*}
& \lim_{x \to 0} \frac{3x^2 - e^{x^2} + \cos (2x)}{x\sin x - \ln (1+x^2)}\\
=& \lim_{x \to 0} \dfrac{\dfrac{1}{6}x^4 + \text{higher order terms}}{\dfrac{1}{3}x^4 + \text{higher order terms}} \\
=& \dfrac{\dfrac{1}{6}}{\dfrac{1}{3}} = \frac{1}{2}
\end{align*}


\newpage
\section{Taylor applications: adding series}
\eg 1: compute \(A = \sum_{n = 1}^{\infty} \frac{n}{2^n}\)
\begin{itemize}
    \item Want \DS{\sum_{n = 1}^{\infty} n x^n} when \(x = \frac{1}{2}\). This is almost, but not quite, the geometric series---\(\sum_{n = 0}^{\infty} x^n\), except there's an additional \(n\). \begin{align*}
    \sum_{n = 1}^{\infty} n x^{n-1} &= \frac{d}{dx} \sum_{n = 0}^{\infty} x^n  = \frac{d}{dx}\frac{1}{1-x} = \frac{1}{(1-x)^2} \\
    \sum_{n = 1}^{\infty} nx^n &= \frac{x}{(1-x)^2}
    \end{align*}
    \item Evaluate at \(x = \frac{1}{2}: \sum_{n = 1}^{\infty} n \frac{1}{2^n} = \frac{\frac{1}{2}}{(1-\frac{1}{2})^2} = 2\)
\end{itemize}
\eg 2: compute \(B = \sum_{n = 0}^{\infty} \frac{2^n}{(n+2) n! }\)
\begin{itemize}
    \item Want \DS{\sum_{n = 0}^{\infty} \frac{x^n}{(n+2)n!}} when \(x = 2\).
    \item Know \DS{\sum_{n = 0}^{\infty} \frac{x^n}{n!} = e^x}.
    \begin{align*}
    x^{n+1} &= \frac{x^{n+2}}{(n+2)}\\
    \int \sum_{n = 0}^{\infty} \frac{x^{n+1}}{n!} dx &= \sum_{n = 0}^{\infty} \frac{x^{n+2}}{(n+2)n!} + C \\
    \int xe^x x = \int \sum_{n = 0}^{\infty} \frac{x^{n+1}}{n!} dx &= \sum_{n = 0}^{\infty} \frac{x^{n+2}}{(n+2)n!} + C
   (x-1)e^x & = \sum_{n = 0}^{\infty} \frac{x^{n+2}}{(n+2)n!}  + C \\
   \intertext{Evaluate at \(x = 0: -1 = 0 + C; C = -1\)}
   \sum_{n = 0}^{\infty} \frac{x^{n+2}}{(n+2) n!} & = (x-1)e^x + 1 \\
   \sum_{n = 0}^{\infty} \frac{x^{n}}{(n+2) n!} & = \frac{ (x-1)e^x + 1 }{x^2} \\
   \intertext{Evaluate at \DS{x = 2: \sum_{n = 0}^{\infty} \frac{2^n}{(n+2)n!} = \frac{e^2 + 1}{4}}}
    \end{align*}
\end{itemize}
\end{document}
